---
title: "Day 5"
author: "Amy Heather"
date: "2024-06-26"
categories: [reproduce]
bibliography: ../../../quarto_site/references.bib
---

::: {.callout-note}

TBC. Total time used: TBCh TBCm (TBC%)

:::

## Times below: Creating Figure 4

Times here (as lots to put in header):

* 9.31-9.49
* 10.04-10.50
* 11.01-11.48
* 11.56-12.02
* 12.50-12.56
* 13.08-

From text, understand this analysis to be variants of configruation one where the number of beds is increased by 1 or 2, and where the number of childbirth arrivals is 1, 1.5 or 2 per day.

Working out IAT:

    * 1 = IAT 1440 (as in e.g. config1)
    * 1.5 = 960 (as 720+240=960, and 960+480=1440)
    * 2 = IAT 720 (as we did for Figure 3)

As for others, just run 10 replications. The paper states they run 100, but that takes ages, and so I've taken the approach that - if it seems reasonably similar enough with 10 replications - I'm happy with it at this point. For posterity, can run all with 100 now or at the end if desired. As these just present mean results, I would only expect it to have a fairly minimal impact for mean of 10 v.s. 100 replications. For Table 6, I did run 100 replications as it was mean and SD.

However, this is not correct:

![](fig4_incorrect.png)

Realised this was my mistake and I had mis-interpreted it as adding 1 or 2 extra beds, but instead, it was simply having 1 or 2 beds in total. Upon re-reading the text again, this was definitely my mistake and not the article. Upon amending this, the figure looks much closer.

![](fig4_close.png)

I was reasonably confident in this being correct, but anticipate the differences here would likely be resolved by doing 100 replications rather than 10. As such, tried running more replications (25 still instead of 100 due to time). Notebook run time 7m 16s.

This brought the result closer, although still some small differences.

![](fig4_25.png)

As the figure is labelled, I can compare the numbers:

```{python}
import pandas as pd

# Input results from original and reproduced figures
comp = pd.DataFrame({
    'original': [0.021, 0.039, 0.0599, 0.150, 0.195, 0.270],
    'reproduced': [0.019, 0.036, 0.060, 0.157, 0.218, 0.276]})

# Find size of difference and % change
comp['diff'] = comp['reproduced'] - comp['original']
comp['pct_change'] = comp.pct_change(axis=1).iloc[:, 1]*100
comp
```

They're very similar, and the only difference I'm uncertain about is 1 bed with 1.5 arrivals, as the actual size of the difference is 0.023 (19.5% referred elsewhere v.s. 21.8% referred elsewhere).

However, I think it is honestly pretty similar, and given that all the other points are very close. I tried running with 10 replications again to see if it looked particularly different, but came out as 0.221.

Since everything else looks right, I guessed it could potentially be related to the IAT used for 1.5 births. Although I'm pretty confident in my estimated value, I decided to try running it with a different IAT - just using a value equidistant between 720 and 1440. When I did this, we got a much closer result for the 1 bed, but a more different result for the 2 bed, for 1.5 births.

![](fig4_1080.png)

I again decided to try upping this to 25 replications to see if that has an impact.

![](fig4_1080_25.png)

Due to the differing results observed, I decided to run both IATs for 100 replications and use the results from that that best matched the original paper. However, this was heating up the computer and VS Code crashed at 80 replications (before any results are saved). I then tried 25 replications, but this experienced an error and seemed to get stuck running one of the models after that - 

```
Traceback (most recent call last):
  File "/home/amy/mambaforge/envs/shoaib2022/lib/python3.9/site-packages/salabim/salabim.py", line 4784, in step
    next(c._process)
  File "/home/amy/Documents/stars/stars-reproduce-shoaib-2022/reproduction/PHC.py", line 290, in process
    yield self.hold(sim.Normal(5,1).bounded_sample())
  File "/home/amy/mambaforge/envs/shoaib2022/lib/python3.9/site-packages/salabim/salabim.py", line 12058, in hold
    self._reschedule(scheduled_time, priority, urgent, "hold")
  File "/home/amy/mambaforge/envs/shoaib2022/lib/python3.9/site-packages/salabim/salabim.py", line 11827, in _reschedule
    raise ValueError("scheduled time ({:0.3f}) before now ({:0.3f})".format(scheduled_time, self.env._now))
ValueError: scheduled time (91194.770) before now (91195.054)
```

As that had taken 15 minutes and also alot of compute power, I decided to go with the simplest approach: to run again but with 10 replications, as I have done for prior figures (for consistency with prior figures, and as I know from experimenting between 10 and 25 replications, that often this only has a fairly small impact on the mean values observed).

::: {.callout-tip}
## Reflections

Found it a bit tricky to work out to go from number of arrivals to inter-arrival rate. Still not quite sure on how, beyond making estimates based on provided arrivals and IAT.

Models that take a long time to run are a bit trickier to work with as it limits what you can do while you wait

Have assumed here that potentially the result might be from two IAT (cannot guarnatee that to be the case, but is what enabled me to get more consistent results with original paper). Regardless of what is true in this case (as we cannot necessarily know), a learning here is that if you do directly change parameters in a script for scenarios, it potentially leaves more margin for error than if it is done programmatically.
:::

## Timings

```{python}
import sys
sys.path.append('../')
from timings import calculate_times

# Minutes used prior to today
used_to_date = 1012

# Times from today
times = [
    ('9.31', '9.49'),
    ('10.04', '10.50'),
    ('11.01', '11.48'),
    ('11.56', '12.02'),
    ('12.50', '12.56')
]

calculate_times(used_to_date, times)
```