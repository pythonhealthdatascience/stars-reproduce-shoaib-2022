---
title: "Day 3"
author: "Amy Heather"
date: "2024-06-24"
categories: [reproduce]
bibliography: ../../../quarto_site/references.bib
---

::: {.callout-note}

Succesful reproduction of Table 6, and start on figure 2. Total time used: 13h 49m (34.5%)

:::

Any references to the original study are to @shoaib_simulation_2021 which is licensed under [CC BY-NC-ND](http://creativecommons.org/licenses/by-nc-nd/4.0/).

## 9.20-10.03, 10.11-10.22, 10.31-11.43: Different configurations

Got parameters for each configuration from Table 3. Some discrepancies:

* Configuration 4 (benchmark) is stated as 4 nurses in the table, but in the text, is described as having 5 nurses (1 NCD and 4 staff) - although the staff nurses are described as working alone in consecutive 8 hour shifts, which would work out to 3 nurses, so it is assumed that the standard configuration of 3 staff nurse and 1 NCD nurse (and so 4 nurses overall, as in the table) is used.
* Table 3 caption describes all configurations as having 6 inpatient and 1 labour bed - whilst Table 6 describes configuration 3 as having 0 labour room beds. However, as there is no labour arrivals, this has no impact on results, so just left as default 1 bed as per Table 3.

Configuration 3 has no arrivals for childbirth or ANC, and so Table 3 states their inter-arrival time as NaN. However, it was unclear how to set this up in the model code.

Trying to figure how best to re-run the `PHC.py` script given it is set up with global variables. Based on [this StackOverflow post](https://stackoverflow.com/questions/64283418/setting-back-global-variables-back-to-their-default-value), tried out creating all global parameters using a single class, so that - if you want to re-run and reset all global parameters - you just make a new instance of this class. Like this -

```
DEFAULT_OPD_IAT = 4
DEFAULT_DELIVERY_IAT = 1440
DEFAULT_IPD_IAT = 2880
DEFAULT_ANC_IAT = 1440


class ModelState:
    """
    Class containing all global parameters (so that, if you wish to reset
    values, you can do so easily by creating a new instance of the ModelState)
    """
    def __init__(self,
                 OPD_iat=DEFAULT_OPD_IAT,
                 delivery_iat=DEFAULT_DELIVERY_IAT,
                 IPD_iat=DEFAULT_IPD_IAT,
                 ANC_iat=DEFAULT_ANC_IAT):
        """
        Parameters for the simulation model.

        Parameters:
        -----------
        OPD_iat : int
            Inter-arrival time for OPD patients

        delivery_iat : int
        `   Inter-arrival time for labour patients

        IPD_iat : int
            Inter-arrival time for IPD patients

        ANC_iat : int
            Inter-arrival time for ANC patients
        """
        self.OPD_iat = OPD_iat
        self.delivery_iat = delivery_iat
        self.IPD_iat = IPD_iat
        self.ANC_iat = ANC_iat


c1_state = ModelState()
c1_state.__dict__

c2_state = ModelState()
c2_state.OPD_iat = 9
c2_state.__dict__

c3_state = ModelState(OPD_iat=5)
c3_state.__dict__
```

But then reflected that this was changing the code alot, and I wasn't 100% confident on whether it would definitely deal with all the global variables correctly (as they are repeatedly defined throughout the script).

Chat with Tom and he suggested:

* `dir()` should show global variables - but these just appear to be for notebook, so hopefully no overlap, but to check -
* Return the value of the global variables after running the model. Then run the model again, and check the values are the same
* He also suggested, for when there are no arrivals for two patient types, the simplest way to set this up is to set the inter-arrival time to something really really large (e.g. 10,000,000 * run time)

I created a function to return global values:

```
def return_globals():
    """
    Return global variables and their values from workspace. Used for
    verifying the environment is as expected.

    Returns:
    -------
    globals() : dict
        Dictionary of global variables and their values
    """
    return globals()
```

And then checked if the output was consistent between different calls of the model, and this returned True, so will assume that running the model in this way is not leading to results interefering with each other between runs. See [GitHub commit 16f50f7](https://github.com/pythonhealthdatascience/stars-reproduce-shoaib-2022/blob/16f50f70a9dac0b3f15cada2bbc3965742c18326/reproduction/reproduce.ipynb) for notebook with these results.

## 12.00-12.05, 12.17-12.57: Different configurations (continued)

Ran the four different configurations (excluded run time from timings).

Kept setting IAT for ANC higher as was getting an average of 1 arrival (but want to be getting no arrivals), but even with IAT of `999999999999999999999999999999999*365*60*24`, was still getting 1 ANC arrival with every run. Tried setting ANC() and Delivery() to comments in `PHC.py`, and this worked to prevent arrivals, so add True/False inputs to `main()` for whether these functions are called, instead of setting high IAT.

Found differences were:

* Mean OPD queue length was very different for all configuration compared with Table 6.
* Lots of the benchmark case results looked very different to Table 6.

Spent a while looking for why this might be, but not yet figured out.

## 13.55-14.30: Problem-solving benchmark model

To ensure it is not an error I've introduced, I copied the original `PHC.py` and modified it directly (the only change from config 1 that I have identified from the paper is that the OPD IAT is 3 instead of 4), and to produce both results spreadsheets. See [GitHub commit 892cca4](https://github.com/pythonhealthdatascience/stars-reproduce-shoaib-2022/blob/892cca42cb4edf94854cc037ec273a991e6b3544/logbook/posts/2024_06_24/index.qmd). This came out looking the same as my other run (e.g. doctor utilisation around 0.31 instead of 1.14).

Hence, wondered if the issue was in parameterising of model, so looked back over the paper. It is described as differing from the archetypal/generic model in two ways (not just one). 
I have only changed outpatient load (OPD IAT 4 to 3)

> 'Note that this benchmark configuration differs from the archetypal configuration only in the outpatient load and the doctorâ€™s consultation time for outpatients.'
, and had not spotted change in consultation time in the tables.
> 
> The consultation time for the benchmark model: "we have assumed the consultation time to be normally distributed with a mean of 5 minutes and standard deviation of 1 minute with a lower bound fixed at 2 minutes"
>
> As in Table 4, the normal OPD consultation time is normally distribution with mean 0.87 and standard deviation of 0.21

Amended mean and sd, and set consultation boundary as a modifiable parameter (with consult_boundary_1 and 2, as the default boundary provided in the script varied depending on whether it was warm-up or not, between 0.5 and 0.3).

This pretty much fixed the issue - the only large difference for the benchmark model is now as for the other configurations: mean OPD queue length.

## 14.36-14.53: Problem-solving mean OPD queue length

Checked example of a result in full `.xlsx` output for config 1 from a [previous commit](https://github.com/pythonhealthdatascience/stars-reproduce-shoaib-2022/blob/aa79865a880bc29b2b89dd6729a062d797b6deb5/reproduction/outputs/full_results.xlsx), but this matched the model output, which itself, was very similar to the paper output.

Hence, instead ran config 4 but enabled full result spreadsheet to be produced. That output `0.833` for `Mean length of OPD queue`, similar to Table 6's `0.817` and unlike my previously calculated `6.932062`.

Looking at the replication spreadsheet, I realised there were two OPD q len rows:

* `OPD q len` (with values around 7)
    * Calculated from `OPD_q__list`
    * That is created by `OPD_q__list.append(np.mean(an_list))`, and `an_list` is delivery patients
* `opd q len` (with values around 0.8)
    * Calculated from `OPD_q_length_list`.
    * That is created by `OPD_q_length_list.append(waitingline_OPD.length.mean())`

Hence, given the values match, and how it was created, the latter is evidently the appropriate row to use. Amended `reproduce.ipynb` to use that row, and now results match up, with no concerning deviation.

## 15.34-16.02 : Add SD to Table 6

Add standard deviation. Some percent change seem large as it's comparing very small number - but actual change between the model and table 6 all are small enough that I feel happy to say this has been successfully reproduced for the mean values.

Will run 100 replications later, and confirm whether standard deviations are similar - but anticipate to take a while, so will work on something else for remainder of day, as not got time to run them all now.

As the long run-time prevents me from checking this now, I will not yet timestamp this as complete.

## 16.13-17.00: Starting on Figure 2A-D

Figure 2 presents results from sensitivity analysis of configuration 1.

It varies:

* Number of outpatients per day:
    * 170 (same as config 4) = OPD IAT 3
    * 85
    * 65
* Average service time for outpatients - mean (SD):
    * 0.87 (0.21) (same as config 1)
    * 2.5 (0.5)
    * 5 (1)

It states that "estimation of the outpatient arrival rate for this configuration is described in detail in Section 3.3.1". Hence, looked back at provided times and patient counts in the Tables...

* Config 1: 130 arrivals in a day and IAT of 4
* Config 2/3: 60 arrivals in a day and IAT of 9
* Config 4: 170 arrivals in a day and IAT of 3

In order to get those times:

```{python}
print((60/4)*8.6666666666666666666666666666666)
print((60/9)*9)
print((60/3)*8.5)
```

Can see that, in order to get each of the times, the hours per day vary from 8.5 to 9. As such, it is unclear how I should calculate IAT for 85 and 65 per day, due to this variation. It is assumed that this is likely related to rounding?

Then looked to section 3.3.1 about this in the paper, @shoaib_simulation_2021:

> "The average interarrival times (and consequently the average number of patients) at each configuration were estimated in the following manner. The number of outpatients visiting configuration 1 PHCs (PHCs 1, 5, and 9) range from 80 to 150 patients per day. These include patients visiting for the first time for a given case of illness as well as patients visiting for follow-up consultations on a previous case. Thus we assumed that approximately 125 patients visit on a given day for these PHC configurations, which include 90 first-visit patients, 20% patients on their first follow-up, and 10% visiting for their second follow-up, yield approximately 126 patients. Therefore, the interarrival time of 4 minutes at configuration 1 PHCs corresponds to first-time visits, with follow-up visits scheduled at the same time on any day between the next 3 and 8 days."

These values differ from the table - so looking again with these:

* Config 1: 125 or 126 arrivals and IAT 4
* Config 2/3: 55 arrivals and IAT 9

```{python}
print((60/4)*8.33333333333333333)
print((60/4)*8.4)
print((60/9)*8.25)
```

Regarding the number of hours described in the article, this is described as "6 hours per day" for the outpatients - which differs to the 8.25 hours to 9 that align with the provided patient numbers and inter-arrival times.

Assuming this variation is simply related to rounding the IAT - as if we calculate these values based on there being 8.5 hours per day then round the IAT, we get the same results...

```{python}
# IAT 3. Arrivals 170
print(60/(170/8.5))

# IAT 4. Arrivals 125-130
print(60/(125/8.5))
print(60/(130/8.5))

# IAT 9. Arrivals 55-60
print(60/(55/8.5))
print(60/(60/8.5))
```

As such, the inter-arrival times used were calculated as 3, 6 and 8...

```{python}
# 170, 85 and 65 arrivals
print(60/(170/8.5))
print(60/(85/8.5))
print(60/(65/8.5))
```

## Reflections from troubleshooting today

::: {.callout-tip}
## Challenges today

Adapting model to run model programmatically (rather than having to directly change the parameters in the script itself)

Difficulty identifying parameters of scenarios (when not all captured in a single table, but in combination with article text) (and when not provided in the script).

Time spent processing results into desired tables and/or figures (when not already created by the script).

Uncertainty on model parameters if not provided and calculation unclear (inter-arrival time).
:::

## Timings

```{python}
import sys
sys.path.append('../')
from timings import calculate_times

# Minutes used prior to today
used_to_date = 531

# Times from today
times = [
    ('9.20', '10.03'),
    ('10.11', '10.22'),
    ('10.31', '11.43'),
    ('12.00', '12.05'),
    ('12.17', '12.57'),
    ('13.55', '14.30'),
    ('14.36', '14.53'),
    ('15.34', '16.02'),
    ('16.13', '17.00')]

calculate_times(used_to_date, times)
```